{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "import src.config as config\n",
    "import src.utils as utils\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "lev_index   = 0\n",
    "kfold_index = 0\n",
    "experiment_name = \"cv_samplestandardized\"\n",
    "datetime_string = datetime.datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"conv_level_{}_kfold_{}_date_{}\".format(lev_index, kfold_index, datetime_string)\n",
    "model_path = os.path.join(config.model_path, experiment_name, datetime_string,  model_name)\n",
    "os.makedirs(model_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_transform_path = os.path.join(config.data_pro_path,\"ml_transform\", experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_filename = \"train_x_lev_{}_{}.nc\".format(lev_index, kfold_index)\n",
    "valid_x_filename = \"valid_x_lev_{}_{}.nc\".format(lev_index, kfold_index)\n",
    "\n",
    "train_y_filename = \"train_y_{}_{}.nc\".format(lev_index, kfold_index)\n",
    "valid_y_filename = \"valid_y_{}_{}.nc\".format(lev_index, kfold_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_xr = xr.open_dataset(os.path.join(ml_transform_path, train_x_filename))\n",
    "valid_x_xr = xr.open_dataset(os.path.join(ml_transform_path, valid_x_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_xr = xr.open_dataset(os.path.join(ml_transform_path, train_y_filename))\n",
    "valid_y_xr = xr.open_dataset(os.path.join(ml_transform_path, valid_y_filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Stacking dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_xr_stack = train_x_xr.stack(sample=(\"realization\",\"time\"))\n",
    "valid_x_xr_stack = valid_x_xr.stack(sample=(\"realization\",\"time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_xr_stack = train_y_xr.stack(sample=(\"realization\",\"time\"))\n",
    "valid_y_xr_stack = valid_y_xr.stack(sample=(\"realization\",\"time\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sample_coords = valid_y_xr_stack.sample\n",
    "train_sample_coords = train_y_xr_stack.sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Transform to numpy and mask nans with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_ml_np = np.nan_to_num(np.expand_dims(train_x_xr_stack[\"rho\"].transpose(\"sample\",...).values,3),0)\n",
    "valid_x_ml_np = np.nan_to_num(np.expand_dims(valid_x_xr_stack[\"rho\"].transpose(\"sample\",...).values,3),0)\n",
    "\n",
    "train_y_ml_np = train_y_xr_stack[\"atlantic_moc\"].values\n",
    "valid_y_ml_np = valid_y_xr_stack[\"atlantic_moc\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = train_x_xr_stack.lon\n",
    "lat = train_x_xr_stack.lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(30,(3,3), activation=\"relu\", padding=\"same\",input_shape=(120,121,1)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(30,(3,3), activation=\"relu\", padding=\"same\", input_shape=(60,60,8)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(30,(3,3), activation=\"relu\", padding=\"same\", input_shape=(30,30,32)))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(50, activation=\"relu\"))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "#model.add(tf.keras.layers.Dropout(0.2))\n",
    "#model.add(tf.keras.layers.Dense(50, activation=\"relu\"))\n",
    "#model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation=\"linear\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 120, 121, 30)      300       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 60, 60, 30)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 60, 60, 30)        8130      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 30, 30, 30)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 30, 30, 30)        8130      \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 27000)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50)                1350050   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,366,661\n",
      "Trainable params: 1,366,661\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_template = os.path.join(model_path, \"saved-model-{epoch:02d}.hdf5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(model_file_template, monitor='val_mse', verbose=1, save_best_only=False, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class TrainValMSEDiff(Callback):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        train_mse = model.evaluate(train_x_ml_np, train_y_ml_np)\n",
    "        valid_mse = model.evaluate(valid_x_ml_np, valid_y_ml_np)\n",
    "        diff_mse  = train_mse - valid_mse\n",
    "        \n",
    "        logs[\"train_mse_epoch_end\"] = train_mse\n",
    "        logs[\"valid_mse_epoch_end\"] = valid_mse\n",
    "        logs[\"diff_mse_epoch_end\"] = diff_mse\n",
    "        print(f'Epoch {epoch+1} - train_mse: {train_mse:.4f} - val_mse: {valid_mse:.4f} - diff: {diff_mse:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = tf.keras.callbacks.CSVLogger(os.path.join(model_path, \"history.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir = os.path.join(\"logs\", model_name)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = os.path.join(model_path,logdir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model.compile(optimizer = opt, loss=tf.keras.losses.mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13600 samples, validate on 2000 samples\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 10:45:08.741869: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 10:45:08.896763: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-02-28 10:45:08.951552: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  512/13600 [>.............................] - ETA: 1:07 - loss: 391.3251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 10:45:11.404922: I tensorflow/core/profiler/lib/profiler_session.cc:101] Profiler session initializing.\n",
      "2023-02-28 10:45:11.404974: I tensorflow/core/profiler/lib/profiler_session.cc:116] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1024/13600 [=>............................] - ETA: 43s - loss: 387.1126 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 10:45:12.086004: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2023-02-28 10:45:12.087735: I tensorflow/core/profiler/lib/profiler_session.cc:128] Profiler session tear down.\n",
      "2023-02-28 10:45:12.202815: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12\n",
      "\n",
      "2023-02-28 10:45:12.205195: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12/l20407.lvt.dkrz.de.trace.json.gz\n",
      "2023-02-28 10:45:12.217762: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12\n",
      "\n",
      "2023-02-28 10:45:12.219569: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12/l20407.lvt.dkrz.de.memory_profile.json.gz\n",
      "2023-02-28 10:45:12.225621: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12\n",
      "Dumped tool data for xplane.pb to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12/l20407.lvt.dkrz.de.xplane.pb\n",
      "Dumped tool data for overview_page.pb to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12/l20407.lvt.dkrz.de.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12/l20407.lvt.dkrz.de.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12/l20407.lvt.dkrz.de.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/logs/conv_level_23_kfold_0_date_28_02_2023_10_42_25/plugins/profile/2023_02_28_10_45_12/l20407.lvt.dkrz.de.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13600/13600 [==============================] - ETA: 0s - loss: 316.2692"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/keras/engine/training_v1.py:2332: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates = self.state_updates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - train_mse: 171.0116 - val_mse: 4.2940 - diff: 166.7176\n",
      "\n",
      "Epoch 1: saving model to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/saved-model-01.hdf5\n",
      "13600/13600 [==============================] - 73s 5ms/sample - loss: 316.2692 - val_loss: 4.2940 - train_mse_epoch_end: 171.0116 - valid_mse_epoch_end: 4.2940 - diff_mse_epoch_end: 166.7176\n",
      "Epoch 2/200\n",
      "13600/13600 [==============================] - ETA: 0s - loss: 46.8048Epoch 2 - train_mse: 8.0921 - val_mse: 1603.3290 - diff: -1595.2369\n",
      "\n",
      "Epoch 2: saving model to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/saved-model-02.hdf5\n",
      "13600/13600 [==============================] - 59s 4ms/sample - loss: 46.8048 - val_loss: 1603.3290 - train_mse_epoch_end: 8.0921 - valid_mse_epoch_end: 1603.3290 - diff_mse_epoch_end: -1595.2369\n",
      "Epoch 3/200\n",
      "13600/13600 [==============================] - ETA: 0s - loss: 4.3707Epoch 3 - train_mse: 3.2194 - val_mse: 1232.6926 - diff: -1229.4732\n",
      "\n",
      "Epoch 3: saving model to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/saved-model-03.hdf5\n",
      "13600/13600 [==============================] - 62s 5ms/sample - loss: 4.3707 - val_loss: 1232.6926 - train_mse_epoch_end: 3.2194 - valid_mse_epoch_end: 1232.6926 - diff_mse_epoch_end: -1229.4732\n",
      "Epoch 4/200\n",
      "13600/13600 [==============================] - ETA: 0s - loss: 3.1625Epoch 4 - train_mse: 3.0131 - val_mse: 1132.5732 - diff: -1129.5601\n",
      "\n",
      "Epoch 4: saving model to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/saved-model-04.hdf5\n",
      "13600/13600 [==============================] - 65s 5ms/sample - loss: 3.1625 - val_loss: 1132.5733 - train_mse_epoch_end: 3.0131 - valid_mse_epoch_end: 1132.5732 - diff_mse_epoch_end: -1129.5601\n",
      "Epoch 5/200\n",
      "13600/13600 [==============================] - ETA: 0s - loss: 2.9671Epoch 5 - train_mse: 2.9172 - val_mse: 1118.9295 - diff: -1116.0122\n",
      "\n",
      "Epoch 5: saving model to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/saved-model-05.hdf5\n",
      "13600/13600 [==============================] - 62s 5ms/sample - loss: 2.9671 - val_loss: 1118.9295 - train_mse_epoch_end: 2.9172 - valid_mse_epoch_end: 1118.9295 - diff_mse_epoch_end: -1116.0122\n",
      "Epoch 6/200\n",
      "13600/13600 [==============================] - ETA: 0s - loss: 2.8717Epoch 6 - train_mse: 2.8216 - val_mse: 1102.7441 - diff: -1099.9225\n",
      "\n",
      "Epoch 6: saving model to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/saved-model-06.hdf5\n",
      "13600/13600 [==============================] - 61s 5ms/sample - loss: 2.8717 - val_loss: 1102.7442 - train_mse_epoch_end: 2.8216 - valid_mse_epoch_end: 1102.7441 - diff_mse_epoch_end: -1099.9225\n",
      "Epoch 7/200\n",
      "13600/13600 [==============================] - ETA: 0s - loss: 2.7769Epoch 7 - train_mse: 2.7263 - val_mse: 1082.4032 - diff: -1079.6769\n",
      "\n",
      "Epoch 7: saving model to /work/uo1075/u301101/Doktorarbeit/CNN/models/cv_samplestandardized/28_02_2023_10_42_25/conv_level_23_kfold_0_date_28_02_2023_10_42_25/saved-model-07.hdf5\n",
      "13600/13600 [==============================] - 68s 5ms/sample - loss: 2.7769 - val_loss: 1082.4032 - train_mse_epoch_end: 2.7263 - valid_mse_epoch_end: 1082.4032 - diff_mse_epoch_end: -1079.6769\n",
      "Epoch 8/200\n",
      "13600/13600 [==============================] - ETA: 0s - loss: 2.6815"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_x_ml_np, y\u001b[39m=\u001b[39;49mtrain_y_ml_np, batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(valid_x_ml_np, valid_y_ml_np), callbacks \u001b[39m=\u001b[39;49m [TrainValMSEDiff(), checkpoint, csv_logger, tensorboard_callback])\n",
      "File \u001b[0;32m/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/keras/engine/training_v1.py:855\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    854\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 855\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    856\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    857\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    858\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    859\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    860\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    861\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    862\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    863\u001b[0m     validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[1;32m    864\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    865\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    866\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    867\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    868\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    869\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    870\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    871\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    872\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    873\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    874\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    875\u001b[0m )\n",
      "File \u001b[0;32m/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:734\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    729\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`validation_steps` should not be specified if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`validation_data` is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m         )\n\u001b[1;32m    732\u001b[0m     val_x, val_y, val_sample_weights \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m \u001b[39mreturn\u001b[39;00m fit_loop(\n\u001b[1;32m    735\u001b[0m     model,\n\u001b[1;32m    736\u001b[0m     inputs\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    737\u001b[0m     targets\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    738\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weights,\n\u001b[1;32m    739\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    740\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    741\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    742\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    743\u001b[0m     val_inputs\u001b[39m=\u001b[39;49mval_x,\n\u001b[1;32m    744\u001b[0m     val_targets\u001b[39m=\u001b[39;49mval_y,\n\u001b[1;32m    745\u001b[0m     val_sample_weights\u001b[39m=\u001b[39;49mval_sample_weights,\n\u001b[1;32m    746\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    747\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    748\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    749\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    750\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    751\u001b[0m     steps_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps_per_epoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    752\u001b[0m )\n",
      "File \u001b[0;32m/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:482\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m         _reinitialize_iterator(\n\u001b[1;32m    477\u001b[0m             val_iterator, model\u001b[39m.\u001b[39m_distribution_strategy\n\u001b[1;32m    478\u001b[0m         )\n\u001b[1;32m    480\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m ModeKeys\u001b[39m.\u001b[39mTRAIN:\n\u001b[1;32m    481\u001b[0m     \u001b[39m# Epochs only apply to `fit`.\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     callbacks\u001b[39m.\u001b[39;49mon_epoch_end(epoch, epoch_logs)\n\u001b[1;32m    484\u001b[0m \u001b[39m# Reinitialize dataset iterator for the next epoch.\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[39mif\u001b[39;00m reset_dataset_after_each_epoch \u001b[39mand\u001b[39;00m epoch \u001b[39m<\u001b[39m epochs \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/keras/callbacks.py:448\u001b[0m, in \u001b[0;36mCallbackList.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    446\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_logs(logs)\n\u001b[1;32m    447\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 448\u001b[0m     callback\u001b[39m.\u001b[39;49mon_epoch_end(epoch, logs)\n",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m, in \u001b[0;36mTrainValMSEDiff.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_epoch_end\u001b[39m(\u001b[39mself\u001b[39m, epoch, logs\u001b[39m=\u001b[39m{}):\n\u001b[0;32m----> 6\u001b[0m     train_mse \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(train_x_ml_np, train_y_ml_np)\n\u001b[1;32m      7\u001b[0m     valid_mse \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(valid_x_ml_np, valid_y_ml_np)\n\u001b[1;32m      8\u001b[0m     diff_mse  \u001b[39m=\u001b[39m train_mse \u001b[39m-\u001b[39m valid_mse\n",
      "File \u001b[0;32m/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/keras/engine/training_v1.py:974\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mevaluate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    973\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 974\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mevaluate(\n\u001b[1;32m    975\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    976\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    977\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    978\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    979\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    980\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    981\u001b[0m     steps\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m    982\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    983\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    984\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    985\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    986\u001b[0m )\n",
      "File \u001b[0;32m/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:776\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.evaluate\u001b[0;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m batch_size \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_validate_or_infer_batch_size(batch_size, steps, x)\n\u001b[1;32m    767\u001b[0m x, y, sample_weights \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_standardize_user_data(\n\u001b[1;32m    768\u001b[0m     x,\n\u001b[1;32m    769\u001b[0m     y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    774\u001b[0m     steps\u001b[39m=\u001b[39msteps,\n\u001b[1;32m    775\u001b[0m )\n\u001b[0;32m--> 776\u001b[0m \u001b[39mreturn\u001b[39;00m test_loop(\n\u001b[1;32m    777\u001b[0m     model,\n\u001b[1;32m    778\u001b[0m     inputs\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    779\u001b[0m     targets\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    780\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weights,\n\u001b[1;32m    781\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    782\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    783\u001b[0m     steps\u001b[39m=\u001b[39;49msteps,\n\u001b[1;32m    784\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    785\u001b[0m )\n",
      "File \u001b[0;32m/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/keras/engine/training_arrays_v1.py:419\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    414\u001b[0m callbacks\u001b[39m.\u001b[39m_call_batch_hook(\n\u001b[1;32m    415\u001b[0m     mode, \u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m, batch_index, batch_logs\n\u001b[1;32m    416\u001b[0m )\n\u001b[1;32m    418\u001b[0m \u001b[39m# Get outputs.\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m batch_outs \u001b[39m=\u001b[39m f(ins_batch)\n\u001b[1;32m    420\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_outs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    421\u001b[0m     batch_outs \u001b[39m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/keras/backend.py:4577\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4567\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4568\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callable_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4569\u001b[0m     \u001b[39mor\u001b[39;00m feed_arrays \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed_arrays\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4573\u001b[0m     \u001b[39mor\u001b[39;00m session \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_session\n\u001b[1;32m   4574\u001b[0m ):\n\u001b[1;32m   4575\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_callable(feed_arrays, feed_symbols, symbol_vals, session)\n\u001b[0;32m-> 4577\u001b[0m fetched \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callable_fn(\u001b[39m*\u001b[39;49marray_vals, run_metadata\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_metadata)\n\u001b[1;32m   4578\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_fetch_callbacks(fetched[\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fetches) :])\n\u001b[1;32m   4579\u001b[0m output_structure \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mpack_sequence_as(\n\u001b[1;32m   4580\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs_structure,\n\u001b[1;32m   4581\u001b[0m     fetched[: \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutputs)],\n\u001b[1;32m   4582\u001b[0m     expand_composites\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   4583\u001b[0m )\n",
      "File \u001b[0;32m/work/uo1075/u301101/programming/miniconda3/envs/CNN/lib/python3.8/site-packages/tensorflow/python/client/session.py:1481\u001b[0m, in \u001b[0;36mBaseSession._Callable.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1479\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1480\u001b[0m   run_metadata_ptr \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_NewBuffer() \u001b[39mif\u001b[39;00m run_metadata \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1481\u001b[0m   ret \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39;49mTF_SessionRunCallable(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_session\u001b[39m.\u001b[39;49m_session,\n\u001b[1;32m   1482\u001b[0m                                          \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, args,\n\u001b[1;32m   1483\u001b[0m                                          run_metadata_ptr)\n\u001b[1;32m   1484\u001b[0m   \u001b[39mif\u001b[39;00m run_metadata:\n\u001b[1;32m   1485\u001b[0m     proto_data \u001b[39m=\u001b[39m tf_session\u001b[39m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x=train_x_ml_np, y=train_y_ml_np, batch_size=512, epochs=200, validation_data=(valid_x_ml_np, valid_y_ml_np), callbacks = [TrainValMSEDiff(), checkpoint, csv_logger, tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "689bb7d798796eef3aabe566407408febfc13e8fa7d5340267927d1031bb8546"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
