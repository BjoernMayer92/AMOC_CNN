{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.config as config\n",
    "import src.utils as utils\n",
    "from tqdm import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_family = \"GrandEnsemble\"\n",
    "experiment = \"hist\"\n",
    "realization = \"lkm0001\"\n",
    "processing = \"mergetime_rho_remap_masked_ym\"\n",
    "\n",
    "filename_dict = dict(experiment_family = experiment_family,\n",
    "experiment = experiment,\n",
    "realization = realization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"cv_samplestandardized_20yr_split\"\n",
    "\n",
    "output_dir = os.path.join(config.data_pro_path, \"ml_transform\", experiment_name)\n",
    "os.makedirs(output_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "realization_id_list = np.arange(1, 100+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "realization_list = [\"lkm{}\".format(str(realization_id).zfill(4)) for realization_id in realization_id_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_realization_list(realization_list, filename_dict, lev_index):\n",
    "    file_list = []\n",
    "    data_list = []\n",
    "    for realization in tqdm(realization_list):\n",
    "        filename_dict[\"realization\"] = realization\n",
    "        filename_dict[\"processing\"] = \"mergetime_rho_remap_masked_ym\"\n",
    "\n",
    "        path, filename = utils.gen_absolute_path_and_filename(\n",
    "        filename_dict = filename_dict,\n",
    "        init_path = config.data_pro_path, \n",
    "        init_filestem =\"\", \n",
    "        filetype=\"nc\" \n",
    "        )\n",
    "\n",
    "        file = os.path.join(path, filename)\n",
    "        assert os.path.exists(file), \"file {} does not exist\".format(file)\n",
    "        data = xr.open_dataset(file)[\"rho\"].assign_coords({\"realization\": realization}).isel(lev=lev_index)\n",
    "        data_list.append(data)\n",
    "        \n",
    "    return xr.concat(data_list, dim=\"realization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_amoc_realization_list(realization_list, filename_dict):\n",
    "    file_list = []\n",
    "    data_list = []\n",
    "    for realization in tqdm(realization_list):\n",
    "        filename_dict[\"realization\"] = realization\n",
    "        filename_dict[\"processing\"] = \"amoc_ym\"\n",
    "        path, filename = utils.gen_absolute_path_and_filename(\n",
    "        filename_dict = filename_dict,\n",
    "        init_path = config.data_pro_path, \n",
    "        init_filestem =\"\", \n",
    "        filetype=\"nc\" \n",
    "        )\n",
    "\n",
    "        file = os.path.join(path, filename)\n",
    "        assert os.path.exists(file), \"file {} does not exist\".format(file)\n",
    "        data = xr.open_dataset(file)[\"atlantic_moc\"].assign_coords({\"realization\": realization})\n",
    "        data_list.append(data)\n",
    "        \n",
    "    return xr.concat(data_list, dim=\"realization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev_index=23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [02:33<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "data      = load_realization_list(realization_list, filename_dict, lev_index=lev_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:08<00:00, 11.59it/s]\n"
     ]
    }
   ],
   "source": [
    "data_amoc = load_amoc_realization_list(realization_list, filename_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev = data.lev\n",
    "lon = data.lon\n",
    "lat = data.lat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Stack Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_window = 20\n",
    "years = data.time.dt.year.values\n",
    "\n",
    "year_window_list = np.array_split(years, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_split_window_list = [years[20*i:20*(i+1)] for i in range(math.floor(len(years)/n_window))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_split_combinations = list(itertools.combinations(year_window_list, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = split_combination[0]\n",
    "valid_data = split_combination[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, split_combination in enumerate(period_split_combinations):\n",
    "    \n",
    "    train_year_list = split_combination[0]\n",
    "    valid_year_list = split_combination[1]\n",
    "\n",
    "    \n",
    "    train_data = data.where(data.time.dt.year.isin(train_year_list)).dropna(dim=\"time\", how=\"all\")\n",
    "    valid_data = data.where(data.time.dt.year.isin(valid_year_list)).dropna(dim=\"time\", how=\"all\")\n",
    "    \n",
    "    train_data_stack = train_data.stack(sample= (\"realization\",\"time\"))\n",
    "    valid_data_stack = valid_data.stack(sample= (\"realization\",\"time\"))\n",
    "    \n",
    "    train_data_stack_landmasked = train_data_stack.where(train_data_stack!=0)\n",
    "    valid_data_stack_landmasked = valid_data_stack.where(valid_data_stack!=0)\n",
    "    \n",
    "    train_data_stack_samplemean = train_data_stack_landmasked.mean(dim=(\"sample\"))\n",
    "    valid_data_stack_samplemean = valid_data_stack_landmasked.mean(dim=(\"sample\"))\n",
    "\n",
    "    train_data_stack_samplestd = train_data_stack_landmasked.std(dim=(\"sample\"))\n",
    "    valid_data_stack_samplestd = valid_data_stack_landmasked.std(dim=(\"sample\"))\n",
    "    \n",
    "    train_data_stack_sampleanom = train_data_stack_landmasked - train_data_stack_samplemean\n",
    "    valid_data_stack_sampleanom = valid_data_stack_landmasked - train_data_stack_samplemean\n",
    "    \n",
    "    train_data_stack_samplestandardized = train_data_stack_sampleanom/train_data_stack_samplestd\n",
    "    valid_data_stack_samplestandardized = valid_data_stack_sampleanom/train_data_stack_samplestd\n",
    "    \n",
    "\n",
    "    train_data_stack_samplestandardized.unstack().to_netcdf(os.path.join(output_dir,\"train_x_lev_{}_{}.nc\".format(lev_index,i) ))\n",
    "    valid_data_stack_samplestandardized.unstack().to_netcdf(os.path.join(output_dir,\"valid_x_lev_{}_{}.nc\".format(lev_index,i) ))\n",
    "    \n",
    "    \n",
    "    amoc_depth = 1020\n",
    "    amoc_lat = 26.5\n",
    "\n",
    "    train_data_amoc = data_amoc.where(data_amoc.time.dt.year.isin(train_year_list)).dropna(dim=\"time\", how=\"all\")\n",
    "    valid_data_amoc = data_amoc.where(data_amoc.time.dt.year.isin(valid_year_list)).dropna(dim=\"time\", how=\"all\")\n",
    "    \n",
    "    train_data_amoc_depth_1020_lat_26 = train_data_amoc.sel(depth_2 = amoc_depth, lat= amoc_lat).isel(lon=0)/(1025*10**6)\n",
    "    valid_data_amoc_depth_1020_lat_26 = valid_data_amoc.sel(depth_2 = amoc_depth, lat= amoc_lat).isel(lon=0)/(1025*10**6)\n",
    "\n",
    "    #train_data_amoc_depth_1020_lat_26_samplemean = train_data_amoc_depth_1020_lat_26.mean()\n",
    "    #train_data_amoc_depth_1020_lat_26_samplestd = train_data_amoc_depth_1020_lat_26.std()\n",
    "\n",
    "    #train_data_amoc_depth_1020_lat_26_samplestandardized = (train_data_amoc_depth_1020_lat_26 - train_data_amoc_depth_1020_lat_26_samplemean)/train_data_amoc_depth_1020_lat_26_samplestd\n",
    "    #valid_data_amoc_depth_1020_lat_26_samplestandardized = (valid_data_amoc_depth_1020_lat_26 - train_data_amoc_depth_1020_lat_26_samplemean)/train_data_amoc_depth_1020_lat_26_samplestd\n",
    "\n",
    "    #train_data_amoc_depth_1020_lat_26_samplemean.to_netcdf(os.path.join(output_dir,\"train_data_amoc_depth_1020_lat_26_samplemean,nc\"))\n",
    "    #train_data_amoc_depth_1020_lat_26_samplestd.to_netcdf( os.path.join(output_dir, \"train_data_amoc_depth_1020_lat_26_samplestd.nc\"))\n",
    "\n",
    "    train_data_amoc_depth_1020_lat_26.to_netcdf(os.path.join(output_dir, \"train_y_{}_{}.nc\".format(lev_index,i)))\n",
    "    valid_data_amoc_depth_1020_lat_26.to_netcdf(os.path.join(output_dir, \"valid_y_{}_{}.nc\".format(lev_index,i)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN",
   "language": "python",
   "name": "cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "689bb7d798796eef3aabe566407408febfc13e8fa7d5340267927d1031bb8546"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
